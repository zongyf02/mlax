{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM on HuggingFace datasets and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import (\n",
    "    lax,\n",
    "    nn,\n",
    "    random,\n",
    "    numpy as jnp\n",
    ")\n",
    "import optax\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax import Module\n",
    "from mlax.nn import Bias, Embed, Linear, Series, SeriesRng\n",
    "# Local python file containing a bidirectional LSTM layer with output projection.\n",
    "from lstm import BiLSTMBlock"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the SNLI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = load_dataset(\"snli\", cache_dir=\"../data\", split=\"train\").filter(\n",
    "    lambda d: d[\"label\"] != -1\n",
    ")\n",
    "snli_test = load_dataset(\"snli\", cache_dir=\"../data\", split=\"test\").filter(\n",
    "    lambda d: d[\"label\"] != -1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize datasets using a pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "tokenizer = Tokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer.enable_truncation(seq_len)\n",
    "tokenizer.enable_padding(length=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batch):\n",
    "    encodings = tokenizer.encode_batch(\n",
    "        list(zip(batch[\"premise\"], batch[\"hypothesis\"]))\n",
    "    )\n",
    "    batch[\"ids\"] = [encoding.ids for encoding in encodings]\n",
    "    batch[\"type_ids\"] = [encoding.type_ids for encoding in encodings]\n",
    "    batch[\"mask\"] = [\n",
    "        [bool(i) for i in encoding.attention_mask] for encoding in encodings\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_tokenized = snli_train.map(\n",
    "    tokenization, batched=True, batch_size=1024, remove_columns=[\"premise\", \"hypothesis\"], \n",
    ")\n",
    "snli_test_tokenized = snli_test.map(\n",
    "    tokenization, batched=True, batch_size=1024, remove_columns=[\"premise\", \"hypothesis\"]\n",
    ")\n",
    "snli_train_tokenized.set_format(type=\"numpy\")\n",
    "snli_test_tokenized.set_format(type=\"numpy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2146 39\n"
     ]
    }
   ],
   "source": [
    "def numpy_collate(batch):\n",
    "  if isinstance(batch[0], np.ndarray):\n",
    "    return np.stack(batch)\n",
    "  elif isinstance(batch[0], (tuple,list)):\n",
    "    transposed = zip(*batch)\n",
    "    return [numpy_collate(samples) for samples in transposed]\n",
    "  elif isinstance(batch[0], dict):\n",
    "    res = {}\n",
    "    for key in batch[0]:\n",
    "      res[key] = numpy_collate([d[key] for d in batch])\n",
    "    return res\n",
    "  else:\n",
    "    return np.array(batch)\n",
    "\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(\n",
    "    snli_train_tokenized, batch_size, shuffle=True, collate_fn=numpy_collate, num_workers=0\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    snli_test_tokenized, batch_size, collate_fn=numpy_collate, num_workers=0\n",
    ")\n",
    "print(len(train_dataloader), len(test_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01756871  0.02454963  0.005262  ]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self, rng, vocab_size, embed_size=192, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        keys_iter = iter([random.fold_in(rng, i) for i in range(7)])\n",
    "        \n",
    "        self.embed = Embed(next(keys_iter), vocab_size, embed_size)\n",
    "        self.type_embed = Embed(next(keys_iter), 2, embed_size)\n",
    "        self.lstms = SeriesRng([\n",
    "            BiLSTMBlock(next(keys_iter), embed_size, dropout_rate=dropout_rate),\n",
    "            BiLSTMBlock(next(keys_iter), embed_size, dropout_rate=dropout_rate),\n",
    "            BiLSTMBlock(next(keys_iter), embed_size, dropout_rate=dropout_rate)\n",
    "        ])\n",
    "        self.fc = Series([\n",
    "            Linear(next(keys_iter), 3), Bias(next(keys_iter), -1)\n",
    "        ])\n",
    "\n",
    "    def setup(self, xm):\n",
    "        pass\n",
    "\n",
    "    def forward(self, xm, rng, inference_mode=False, batch_axis_name=()):\n",
    "        ids, type_ids, mask = xm\n",
    "        embeddings, self.embed = self.embed(\n",
    "            ids, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        type_embeddings, self.type_embed = self.type_embed(\n",
    "            type_ids, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        embeddings = embeddings + type_embeddings\n",
    "        (activations, _), self.lstms = self.lstms(\n",
    "            (embeddings, mask), rng, inference_mode, batch_axis_name\n",
    "        )\n",
    "        activations = jnp.reshape(activations, (-1,))\n",
    "        activations, self.fc = self.fc(\n",
    "            activations, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        return activations\n",
    "\n",
    "rng1 = random.PRNGKey(0)\n",
    "rng1, rng2 = random.fold_in(rng1, 0), random.fold_in(rng1, 1)\n",
    "model = Model(rng1, tokenizer.get_vocab_size())\n",
    "\n",
    "# Induce lazy initialization\n",
    "for batch in train_dataloader:\n",
    "    ids, type_ids, mask = batch[\"ids\"], batch[\"type_ids\"], batch[\"mask\"]\n",
    "    activations, _ = model(\n",
    "        (ids[0], type_ids[0], mask[0]), rng2, inference_mode=True\n",
    "    )\n",
    "    print(activations)\n",
    "    print(activations.dtype)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(batched_preds, batched_targets):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(\n",
    "        batched_preds, batched_targets\n",
    "    ).mean() # Optax returns per-example loss, this returns the mean batch loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer using Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(learning_rate=8e-5, weight_decay=1e-2)\n",
    "optim_state = optimizer.init(model.filter())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(X, y, rng, model, optim_state):\n",
    "    def _model_loss(X, y, rng, trainables, non_trainables):\n",
    "        model = trainables.combine(non_trainables)\n",
    "        preds, model = jax.vmap(\n",
    "            model.__call__,\n",
    "            in_axes = (0, None, None, None),\n",
    "            out_axes = (0, None),\n",
    "            axis_name = \"N\"\n",
    "        )(X, rng, False, \"N\")\n",
    "        return loss_fn(preds, y), model\n",
    "\n",
    "    # Find batch loss and gradients with resect to trainables\n",
    "    trainables, non_trainables = model.partition()\n",
    "    (loss, model), gradients = jax.value_and_grad(\n",
    "        _model_loss,\n",
    "        argnums=3, # gradients wrt trainables (argument 2 of model_training_loss)\n",
    "        has_aux=True # model is auxiliary data, loss is the true ouput\n",
    "    )(X, y, rng, trainables, non_trainables)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    trainables, non_trainables = model.partition()\n",
    "    gradients, optim_state = optimizer.update(\n",
    "        gradients, optim_state, trainables\n",
    "    )\n",
    "\n",
    "    # Update parameters with new gradients\n",
    "    trainables = optax.apply_updates(gradients, trainables)\n",
    "    return loss, trainables.combine(non_trainables), optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def test_step(X, y, rng, model):\n",
    "    preds, _ = jax.vmap(\n",
    "        model.__call__,\n",
    "        in_axes = (0, None, None, None),\n",
    "        out_axes = (0, None),\n",
    "        axis_name = \"N\"\n",
    "    )(X, rng, True, \"N\")\n",
    "    accurate = (jnp.argmax(preds, axis=1) == y).sum()\n",
    "    return loss_fn(preds, y), accurate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, rng, model, optim_state):\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        _rng = random.fold_in(rng, i)\n",
    "        ids, type_ids, mask = batch[\"ids\"], batch[\"type_ids\"], batch[\"mask\"]\n",
    "        y = batch[\"label\"]\n",
    "        loss, model, optim_state = train_step(\n",
    "            (ids, type_ids, mask), y, _rng, model, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / len(dataloader)}\") \n",
    "    return model, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, rng, model):\n",
    "    test_loss, accurate = 0.0, 0\n",
    "    for batch in dataloader:\n",
    "        ids, type_ids, mask = batch[\"ids\"], batch[\"type_ids\"], batch[\"mask\"]\n",
    "        y = batch[\"label\"]\n",
    "        loss, acc = test_step((ids, type_ids, mask), y, rng, model)\n",
    "        test_loss += loss\n",
    "        accurate += acc\n",
    "\n",
    "    print(f\"Test loss: {test_loss / len(dataloader)}, accuracy: {accurate / len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    rng,\n",
    "    model,\n",
    "    optim_state,\n",
    "    epochs,\n",
    "    test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        _rng = random.fold_in(rng, i)\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model, optim_state = train_epoch(\n",
    "            train_dataloader, _rng, model, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(test_dataloader, _rng, model)\n",
    "        print(f\"----------------\")\n",
    "    return model, optim_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM on the SNLI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 0.9483293294906616\n",
      "Test loss: 0.8295505046844482, accuracy: 0.6239821314811707\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.7915169596672058\n",
      "Test loss: 0.7770448327064514, accuracy: 0.6586930155754089\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.7565377950668335\n",
      "Test loss: 0.7464707493782043, accuracy: 0.6700936555862427\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.7342662215232849\n",
      "Test loss: 0.738709032535553, accuracy: 0.6812907457351685\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.7097809314727783\n",
      "Test loss: 0.7199708819389343, accuracy: 0.6914699077606201\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.6835668087005615\n",
      "Test loss: 0.7135972380638123, accuracy: 0.6993078589439392\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.659674346446991\n",
      "Test loss: 0.6949506998062134, accuracy: 0.7098941802978516\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.6424875259399414\n",
      "Test loss: 0.6857905387878418, accuracy: 0.713660478591919\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.6265079975128174\n",
      "Test loss: 0.6754802465438843, accuracy: 0.7193607687950134\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.6121676564216614\n",
      "Test loss: 0.6808735728263855, accuracy: 0.7168160080909729\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "with jax.default_matmul_precision(\"float32\"):\n",
    "    new_model, new_optim_state = train_loop(\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        rng2,\n",
    "        model,\n",
    "        optim_state,\n",
    "        10, 1\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
