{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Transformer on HuggingFace datasets and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import (\n",
    "    random,\n",
    "    nn,\n",
    "    numpy as jnp\n",
    ")\n",
    "import optax\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax import Module\n",
    "from mlax.nn import (\n",
    "    Embed,\n",
    "    Linear,\n",
    "    Bias,\n",
    "    Series,\n",
    "    SeriesRng\n",
    ")\n",
    "# Local python file containing an \"standard serialized\" encoder block.\n",
    "from encoder import RoPE, EncoderBlock"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the SNLI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = load_dataset(\"snli\", cache_dir=\"../data\", split=\"train\").filter(\n",
    "    lambda d: d[\"label\"] != -1\n",
    ")\n",
    "snli_test = load_dataset(\"snli\", cache_dir=\"../data\", split=\"test\").filter(\n",
    "    lambda d: d[\"label\"] != -1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize datasets using a pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "tokenizer = Tokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer.enable_truncation(seq_len)\n",
    "tokenizer.enable_padding(length=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batch):\n",
    "    encodings = tokenizer.encode_batch(\n",
    "        list(zip(batch[\"premise\"], batch[\"hypothesis\"]))\n",
    "    )\n",
    "    batch[\"ids\"] = [encoding.ids for encoding in encodings]\n",
    "    batch[\"type_ids\"] = [encoding.type_ids for encoding in encodings]\n",
    "    batch[\"mask\"] = [\n",
    "        [bool(i) for i in encoding.attention_mask] for encoding in encodings\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_tokenized = snli_train.map(\n",
    "    tokenization, batched=True, batch_size=1024, remove_columns=[\"premise\", \"hypothesis\"], \n",
    ")\n",
    "snli_test_tokenized = snli_test.map(\n",
    "    tokenization, batched=True, batch_size=1024, remove_columns=[\"premise\", \"hypothesis\"]\n",
    ")\n",
    "snli_train_tokenized.set_format(type=\"numpy\")\n",
    "snli_test_tokenized.set_format(type=\"numpy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2146 39\n"
     ]
    }
   ],
   "source": [
    "def numpy_collate(batch):\n",
    "  if isinstance(batch[0], np.ndarray):\n",
    "    return np.stack(batch)\n",
    "  elif isinstance(batch[0], (tuple,list)):\n",
    "    transposed = zip(*batch)\n",
    "    return [numpy_collate(samples) for samples in transposed]\n",
    "  elif isinstance(batch[0], dict):\n",
    "    res = {}\n",
    "    for key in batch[0]:\n",
    "      res[key] = numpy_collate([d[key] for d in batch])\n",
    "    return res\n",
    "  else:\n",
    "    return np.array(batch)\n",
    "\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(\n",
    "    snli_train_tokenized, batch_size, shuffle=True, collate_fn=numpy_collate, num_workers=0\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    snli_test_tokenized, batch_size, collate_fn=numpy_collate, num_workers=0\n",
    ")\n",
    "print(len(train_dataloader), len(test_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.69678617 -1.653048   -0.1253807 ]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "class Model(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rng,\n",
    "        vocab_size,\n",
    "        seq_len,\n",
    "        embed_size=192,\n",
    "        num_heads=6,\n",
    "        ff_size=768,\n",
    "        act_fn=nn.gelu,\n",
    "        dropout_rate=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        keys_iter = iter([random.fold_in(rng, i) for i in range(7)])\n",
    "\n",
    "        self.embed = Embed(next(keys_iter), vocab_size, embed_size)\n",
    "        self.type_embed = Embed(next(keys_iter), 2, embed_size)\n",
    "        self.rotary = RoPE(seq_len, embed_size // num_heads)\n",
    "        self.encoders = SeriesRng([\n",
    "            EncoderBlock(next(keys_iter), num_heads, ff_size,\n",
    "                         self.rotary.forward, act_fn, dropout_rate),\n",
    "            EncoderBlock(next(keys_iter), num_heads, ff_size,\n",
    "                         self.rotary.forward, act_fn, dropout_rate),\n",
    "            EncoderBlock(next(keys_iter), num_heads, ff_size,\n",
    "                         self.rotary.forward, act_fn, dropout_rate)\n",
    "        ])\n",
    "        self.fc = Series([\n",
    "            Linear(next(keys_iter), 3), Bias(next(keys_iter), -1)\n",
    "        ])\n",
    "    \n",
    "    def setup(self, xm):\n",
    "        pass\n",
    "\n",
    "    def forward(self, xm, rng, inference_mode=False, batch_axis_name=()):\n",
    "        ids, type_ids, mask = xm\n",
    "        embeddings, self.embed = self.embed(\n",
    "            ids, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        type_embeddings, self.type_embed = self.type_embed(\n",
    "            type_ids, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        embeddings = embeddings + type_embeddings\n",
    "\n",
    "        (activations, _), self.encoders = self.encoders(\n",
    "            (embeddings, mask),\n",
    "            random.fold_in(rng, 1), inference_mode, batch_axis_name\n",
    "        )\n",
    "        if mask is not None:\n",
    "            activations = jnp.where(\n",
    "                jnp.expand_dims(mask, axis=(1,)), activations, 0\n",
    "            )\n",
    "\n",
    "        activations = jnp.reshape(activations, (-1,))\n",
    "        activations, self.fc = self.fc(\n",
    "            activations, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        return activations\n",
    "\n",
    "rng1 = random.PRNGKey(0)\n",
    "rng1, rng2 = random.fold_in(rng1, 0), random.fold_in(rng1, 1)\n",
    "model = Model(rng1, tokenizer.get_vocab_size(), seq_len)\n",
    "\n",
    "# Induce lazy initialization\n",
    "for batch in train_dataloader:\n",
    "    ids, type_ids, mask = batch[\"ids\"], batch[\"type_ids\"], batch[\"mask\"]\n",
    "    activations, _ = model(\n",
    "        (ids[0], type_ids[0], mask[0]), rng2, inference_mode=True\n",
    "    )\n",
    "    print(activations)\n",
    "    print(activations.dtype)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(batched_preds, batched_targets):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(\n",
    "        batched_preds, batched_targets\n",
    "    ).mean() # Optax returns per-example loss, this returns the mean batch loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer using Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = 1e-4\n",
    "optimizer = optax.adamw(learning_rate=schedule, weight_decay=1e-2)\n",
    "optim_state = optimizer.init(model.filter())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(X, y, rng, model, optim_state):\n",
    "    def _model_loss(X, y, rng, trainables, non_trainables):\n",
    "        model = trainables.combine(non_trainables)\n",
    "        preds, model = jax.vmap(\n",
    "            model.__call__,\n",
    "            in_axes = (0, None, None, None),\n",
    "            out_axes = (0, None),\n",
    "            axis_name = \"N\"\n",
    "        )(X, rng, False, \"N\")\n",
    "        return loss_fn(preds, y), model\n",
    "\n",
    "    # Find batch loss and gradients with resect to trainables\n",
    "    trainables, non_trainables = model.partition()\n",
    "    (loss, model), gradients = jax.value_and_grad(\n",
    "        _model_loss,\n",
    "        argnums=3, # gradients wrt trainables (argument 2 of model_training_loss)\n",
    "        has_aux=True # model is auxiliary data, loss is the true ouput\n",
    "    )(X, y, rng, trainables, non_trainables)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    trainables, non_trainables = model.partition()\n",
    "    gradients, optim_state = optimizer.update(\n",
    "        gradients, optim_state, trainables\n",
    "    )\n",
    "\n",
    "    # Update parameters with new gradients\n",
    "    trainables = optax.apply_updates(gradients, trainables)\n",
    "    return loss, trainables.combine(non_trainables), optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def test_step(X, y, rng, model):\n",
    "    preds, _ = jax.vmap(\n",
    "        model.__call__,\n",
    "        in_axes = (0, None, None, None),\n",
    "        out_axes = (0, None),\n",
    "        axis_name = \"N\"\n",
    "    )(X, rng, True, \"N\")\n",
    "    accurate = (jnp.argmax(preds, axis=1) == y).sum()\n",
    "    return loss_fn(preds, y), accurate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, rng, model, optim_state):\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        _rng = random.fold_in(rng, i)\n",
    "        ids, type_ids, mask = batch[\"ids\"], batch[\"type_ids\"], batch[\"mask\"]\n",
    "        y = batch[\"label\"]\n",
    "        loss, model, optim_state = train_step(\n",
    "            (ids, type_ids, mask), y, _rng, model, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / len(dataloader)}\") \n",
    "    return model, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, rng, model):\n",
    "    test_loss, accurate = 0.0, 0\n",
    "    for batch in dataloader:\n",
    "        ids, type_ids, mask = batch[\"ids\"], batch[\"type_ids\"], batch[\"mask\"]\n",
    "        y = batch[\"label\"]\n",
    "        loss, acc = test_step((ids, type_ids, mask), y, rng, model)\n",
    "        test_loss += loss\n",
    "        accurate += acc\n",
    "\n",
    "    print(f\"Test loss: {test_loss / len(dataloader)}, accuracy: {accurate / len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    rng,\n",
    "    model,\n",
    "    optim_state,\n",
    "    epochs,\n",
    "    test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        _rng = random.fold_in(rng, i)\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model, optim_state = train_epoch(\n",
    "            train_dataloader, _rng, model, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(test_dataloader, _rng, model)\n",
    "        print(f\"----------------\")\n",
    "    return model, optim_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Encoder on the SNLI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0934607982635498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8719746470451355, accuracy: 0.6052524447441101\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.802844226360321\n",
      "Test loss: 0.7583152651786804, accuracy: 0.6616449952125549\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.7447420954704285\n",
      "Test loss: 0.7323169112205505, accuracy: 0.6855660080909729\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.7055930495262146\n",
      "Test loss: 0.6948862671852112, accuracy: 0.703888475894928\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.6646700501441956\n",
      "Test loss: 0.6679456830024719, accuracy: 0.7207858562469482\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.6290363669395447\n",
      "Test loss: 0.6589967608451843, accuracy: 0.7273005247116089\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.6009421944618225\n",
      "Test loss: 0.6434420347213745, accuracy: 0.7369707226753235\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.57776939868927\n",
      "Test loss: 0.6396872997283936, accuracy: 0.735850989818573\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.5565153360366821\n",
      "Test loss: 0.6439101099967957, accuracy: 0.7432817816734314\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.5364888310432434\n",
      "Test loss: 0.6255167126655579, accuracy: 0.7494910955429077\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "with jax.default_matmul_precision(\"float32\"):\n",
    "    new_model, new_optim_state = train_loop(\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        rng2,\n",
    "        model,\n",
    "        optim_state,\n",
    "        10, 1\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c3d7272c1eba356ec9149ec42daf5acdf55d6fdb447aefce6509807a5e73802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
